{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9779709,"sourceType":"datasetVersion","datasetId":5991214},{"sourceId":9794107,"sourceType":"datasetVersion","datasetId":6001830}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BartTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments\n\n# Load the datasets\ntrain_df = pd.read_csv('/kaggle/input/hs-cs-kn/hscnkp_train.csv')  # Adjust file name for training data\nvalid_df = pd.read_csv('/kaggle/input/hs-cs-kn-valid/hscnkp_valid.csv')  # Adjust file name for validation data\ntest_df = pd.read_csv('/kaggle/input/hs-cs-kn/hscnkp_test.csv')    # Adjust file name for test data\ntrain_df.dropna(inplace=True)\nvalid_df.dropna(inplace=True)\ntest_df.dropna(inplace=True)\n\n# Define custom tokens\nHS_TOKEN = \"<HS>\"\nKN_TOKEN = \"<KN>\"\nCN_TOKEN = \"<CN>\"\n\n# Prepare the input data (without the counter-narrative in the input)\ntrain_df['input'] = HS_TOKEN + train_df['hate_speech'] + KN_TOKEN + train_df['knowledge_sentence']\nvalid_df['input'] = HS_TOKEN + valid_df['hate_speech'] + KN_TOKEN + valid_df['knowledge_sentence']\ntest_df['input'] = HS_TOKEN + test_df['hate_speech'] + KN_TOKEN + test_df['knowledge_sentence']\n\n# Convert counter-narrative into the output (target)\ntrain_df['output'] = CN_TOKEN + train_df['counter_narrative']\nvalid_df['output'] = CN_TOKEN + valid_df['counter_narrative']\n\n# Convert to lists\ntrain_inputs = train_df['input'].tolist()\ntrain_outputs = train_df['output'].tolist()\nvalid_inputs = valid_df['input'].tolist()\nvalid_outputs = valid_df['output'].tolist()\ntest_inputs = test_df['input'].tolist()\n\nclass CustomDataset(Dataset):\n    def __init__(self, inputs, outputs, tokenizer):\n        self.inputs = inputs\n        self.outputs = outputs\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.inputs)\n\n    def __getitem__(self, idx):\n        # Tokenize input and output separately\n        input_encoding = self.tokenizer(\n            self.inputs[idx], \n            truncation=True, \n            padding='max_length', \n            max_length=512,\n            return_tensors='pt'\n        )\n        output_encoding = self.tokenizer(\n            self.outputs[idx], \n            truncation=True, \n            padding='max_length', \n            max_length=512,\n            return_tensors='pt'\n        )\n\n        input_ids = input_encoding['input_ids'].squeeze()\n        attention_mask = input_encoding['attention_mask'].squeeze()\n        labels = output_encoding['input_ids'].squeeze()\n\n        return {\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n            'labels': labels\n        }\n\n# Load tokenizer and model\nmodel_name = 'facebook/bart-base'\ntokenizer = BartTokenizer.from_pretrained(model_name)\nmodel = BartForConditionalGeneration.from_pretrained(model_name)\n\n# Add special tokens to the tokenizer\nspecial_tokens_dict = {\n    'additional_special_tokens': [HS_TOKEN, KN_TOKEN, CN_TOKEN]\n}\nnum_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n\n# Resize the model's token embeddings to accommodate new tokens\nmodel.resize_token_embeddings(len(tokenizer))\n\n# Create datasets (note we are now passing both inputs and outputs)\ntrain_dataset = CustomDataset(train_inputs, train_outputs, tokenizer)\nvalid_dataset = CustomDataset(valid_inputs, valid_outputs, tokenizer)\n\n# Set up training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',                # Output directory\n    num_train_epochs=3,                    # Number of training epochs\n    per_device_train_batch_size=4,         # Batch size for training\n    per_device_eval_batch_size=4,          # Batch size for evaluation\n    warmup_steps=500,                       # Warmup steps\n    weight_decay=0.01,                     # Strength of weight decay\n    logging_dir='./logs',                   # Directory for storing logs\n    logging_steps=10,                       # Log every 10 steps\n    evaluation_strategy=\"epoch\",            # Evaluate every epoch\n    save_strategy=\"epoch\",                  # Save model at the end of each epoch\n    save_total_limit=2,                     # Limit the total amount of checkpoints\n    load_best_model_at_end=True,            # Load the best model when finished training\n    metric_for_best_model=\"loss\",           # Specify the metric to use to compare models\n    report_to=[],                           # Disable wandb\n)\n\n# Initialize the Trainer\ntrainer = Trainer(\n    model=model,                            # The instantiated ðŸ¤— Transformers model to be trained\n    args=training_args,                     # Training arguments, defined above\n    train_dataset=train_dataset,            # Training dataset\n    eval_dataset=valid_dataset              # Validation dataset\n)\n\n# Train the model\ntrainer.train()\n\n# Save the model and tokenizer after training\ntrainer.save_model('./results/fine-tuned-bart')\ntokenizer.save_pretrained('./results/fine-tuned-bart')\n\n# To generate counter narratives, use the model after training:\nmodel.eval()  # Switch model to evaluation mode\n\n# Example of generating counter narratives for inputs from the test set\nfor example_input in test_inputs[:5]:  # Change the range for more inputs\n    input_ids = tokenizer.encode(example_input, return_tensors='pt').to(model.device)\n\n    # Generate output with BART\n    output = model.generate(\n        input_ids, \n        max_length=512,\n        num_return_sequences=2,\n        do_sample=True,  # Enable sampling instead of greedy decoding\n        top_p=0.9,       # Set the top_p value for nucleus sampling\n        temperature=1.0  # You can adjust temperature for more or less randomness\n    )\n\n    # Decode the generated tokens to string\n    generated_counter_narrative = tokenizer.decode(output[0], skip_special_tokens=True)\n    print(f\"Input: {example_input}\\nGenerated Counter Narrative: {generated_counter_narrative}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T19:37:07.533576Z","iopub.execute_input":"2024-11-08T19:37:07.534060Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47ac3650f39e4355a4ce5f7786f4380e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75f8bb83052f4b39973f5d00372a902b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e16549b375a4d8cb2c2efde2a363cde"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.72k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c8fc4c1bbe94df1b18ff9c6a89d7ed0"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10e64217dbcf4b2887c7d16cd1251dd8"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='449' max='1467' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 449/1467 06:03 < 13:48, 1.23 it/s, Epoch 0.92/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# List to store data for the CSV file\noutput_data = []\n\n# Process each input to generate two counter-narratives\nfor example_input in test_inputs:\n    # Extract hate_speech text between <HS> and <KN> tokens\n    hate_speech = example_input.split(HS_TOKEN)[1].split(KN_TOKEN)[0].strip()\n\n    input_ids = tokenizer.encode(example_input, return_tensors='pt').to(model.device)\n\n    # Generate 2 counter-narratives with nucleus sampling\n    outputs = model.generate(\n        input_ids,\n        max_length=512,\n        num_return_sequences=2,  # Generate 2 counter-narratives\n        do_sample=True,          # Enable sampling instead of greedy decoding\n        top_p=0.9,               # Set top_p for nucleus sampling\n        temperature=1.0          # Adjust temperature for randomness\n    )\n\n    # Decode generated counter-narratives\n    counter_narratives = []\n    for output in outputs:\n        generated_text = tokenizer.decode(output, skip_special_tokens=True)\n        counter_narratives.append(generated_text.strip())\n\n    # Append the data to the list\n    output_data.append({\n        'hs': hate_speech,\n        'cn1': counter_narratives[0] if len(counter_narratives) > 0 else \"\",\n        'cn2': counter_narratives[1] if len(counter_narratives) > 1 else \"\"\n    })\n\n# Convert list to DataFrame and save to CSV\noutput_df = pd.DataFrame(output_data)\noutput_df.to_csv('bart-out.csv', index=False)\n\nprint(\"CSV file 'generated_counter_narratives.csv' created successfully.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n\n# Load the datasets\ntrain_df = pd.read_csv('/kaggle/input/hs-cs-kn/hscnkp_train.csv')  # Adjust file name for training data\nvalid_df = pd.read_csv('/kaggle/input/hs-cs-kn-valid/hscnkp_valid.csv')  # Adjust file name for validation data\ntest_df = pd.read_csv('/kaggle/input/hs-cs-kn/hscnkp_test.csv')    # Adjust file name for test data\ntrain_df.dropna(inplace=True)\nvalid_df.dropna(inplace=True)\ntest_df.dropna(inplace=True)\n\n# Define custom tokens (for special task-oriented tokens in the dataset)\nHS_TOKEN = \"<HS>\"\nKN_TOKEN = \"<KN>\"\nCN_TOKEN = \"<CN>\"\n\n# Prepare the input data with instruction to generate the counter-narrative\ntrain_df['input'] = (\n    \"Generate a counter-narrative: \" + HS_TOKEN + train_df['hate_speech'] + KN_TOKEN + train_df['knowledge_sentence']\n)\nvalid_df['input'] = (\n    \"Generate a counter-narrative: \" + HS_TOKEN + valid_df['hate_speech'] + KN_TOKEN + valid_df['knowledge_sentence']\n)\ntest_df['input'] = (\n    \"Generate a counter-narrative: \" + HS_TOKEN + test_df['hate_speech'] + KN_TOKEN + test_df['knowledge_sentence']\n)\n\n# Convert counter-narrative into the output (target)\ntrain_df['output'] = CN_TOKEN + train_df['counter_narrative']\nvalid_df['output'] = CN_TOKEN + valid_df['counter_narrative']\n\n# Convert to lists\ntrain_inputs = train_df['input'].tolist()\ntrain_outputs = train_df['output'].tolist()\nvalid_inputs = valid_df['input'].tolist()\nvalid_outputs = valid_df['output'].tolist()\ntest_inputs = test_df['input'].tolist()\n\nclass CustomDataset(Dataset):\n    def __init__(self, inputs, outputs, tokenizer):\n        self.inputs = inputs\n        self.outputs = outputs\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.inputs)\n\n    def __getitem__(self, idx):\n        # Tokenize input and output separately\n        input_encoding = self.tokenizer(\n            self.inputs[idx], \n            truncation=True, \n            padding='max_length', \n            max_length=512,\n            return_tensors='pt'\n        )\n        output_encoding = self.tokenizer(\n            self.outputs[idx], \n            truncation=True, \n            padding='max_length', \n            max_length=512,\n            return_tensors='pt'\n        )\n\n        input_ids = input_encoding['input_ids'].squeeze()\n        attention_mask = input_encoding['attention_mask'].squeeze()\n        labels = output_encoding['input_ids'].squeeze()\n\n        return {\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n            'labels': labels\n        }\n\n# Load tokenizer and model for FLAN-T5\nmodel_name = 'google/flan-t5-base'  # You can also use 'google/flan-t5-large' for larger models\ntokenizer = T5Tokenizer.from_pretrained(model_name)\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)\n\n# Create datasets (note we are now passing both inputs and outputs)\ntrain_dataset = CustomDataset(train_inputs, train_outputs, tokenizer)\nvalid_dataset = CustomDataset(valid_inputs, valid_outputs, tokenizer)\n\n# Set up training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',                # Output directory\n    num_train_epochs=3,                    # Number of training epochs\n    per_device_train_batch_size=4,         # Batch size for training\n    per_device_eval_batch_size=4,          # Batch size for evaluation\n    warmup_steps=500,                      # Warmup steps\n    weight_decay=0.01,                     # Strength of weight decay\n    logging_dir='./logs',                  # Directory for storing logs\n    logging_steps=10,                      # Log every 10 steps\n    evaluation_strategy=\"epoch\",           # Evaluate every epoch\n    save_strategy=\"epoch\",                 # Save model at the end of each epoch\n    save_total_limit=2,                    # Limit the total amount of checkpoints\n    load_best_model_at_end=True,           # Load the best model when finished training\n    metric_for_best_model=\"loss\",          # Specify the metric to use to compare models\n    report_to=[],                          # Disable wandb\n)\n\n# Initialize the Trainer\ntrainer = Trainer(\n    model=model,                            # The instantiated ðŸ¤— Transformers model to be trained\n    args=training_args,                     # Training arguments, defined above\n    train_dataset=train_dataset,            # Training dataset\n    eval_dataset=valid_dataset              # Validation dataset\n)\n\n# Train the model\ntrainer.train()\n\n# Save the model and tokenizer after training\ntrainer.save_model('./results/fine-tuned-flan-t5')\ntokenizer.save_pretrained('./results/fine-tuned-flan-t5')\n\n# To generate counter narratives, use the model after training:\nmodel.eval()  # Switch model to evaluation mode\n\n# Example of generating counter narratives for inputs from the test set\nfor example_input in test_inputs[:5]:  # Change the range for more inputs\n    input_ids = tokenizer.encode(example_input, return_tensors='pt').to(model.device)\n\n    # Generate output with Flan-T5\n    output = model.generate(\n        input_ids, \n        max_length=512,\n        num_return_sequences=1,\n        do_sample=True,  # Enable sampling instead of greedy decoding\n        top_p=0.9,       # Set the top_p value for nucleus sampling\n        temperature=1.0  # You can adjust temperature for more or less randomness\n    )\n\n    # Decode the generated tokens to string\n    generated_counter_narrative = tokenizer.decode(output[0], skip_special_tokens=True)\n    print(f\"Input: {example_input}\\nGenerated Counter Narrative: {generated_counter_narrative}\\n\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# List to store data for the CSV file\noutput_data = []\n\n# Process each input to generate two counter-narratives\nfor example_input in test_inputs:\n    # Extract hate_speech text between <HS> and <KN> tokens\n    hate_speech = example_input.split(HS_TOKEN)[1].split(KN_TOKEN)[0].strip()\n\n    input_ids = tokenizer.encode(example_input, return_tensors='pt').to(model.device)\n\n    # Generate 2 counter-narratives with nucleus sampling\n    outputs = model.generate(\n        input_ids,\n        max_length=512,\n        num_return_sequences=2,  # Generate 2 counter-narratives\n        do_sample=True,          # Enable sampling instead of greedy decoding\n        top_p=0.9,               # Set top_p for nucleus sampling\n        temperature=1.0          # Adjust temperature for randomness\n    )\n\n    # Decode generated counter-narratives\n    counter_narratives = []\n    for output in outputs:\n        generated_text = tokenizer.decode(output, skip_special_tokens=False)\n        counter_narratives.append(generated_text.strip())\n\n    # Append the data to the list\n    output_data.append({\n        'hs': hate_speech,\n        'cn1': counter_narratives[0] if len(counter_narratives) > 0 else \"\",\n        'cn2': counter_narratives[1] if len(counter_narratives) > 1 else \"\"\n    })\n\n# Convert list to DataFrame and save to CSV\noutput_df = pd.DataFrame(output_data)\noutput_df.to_csv('flanT5-out.csv', index=False)\n\nprint(\"CSV file 'generated_counter_narratives.csv' created successfully.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}